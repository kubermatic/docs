{
  "groups": [
    {
      "name": "blackbox-exporter",
      "rules": [
        {
          "alert": "HttpProbeFailed",
          "annotations": {
            "message": "Probing the blackbox-exporter target {{ $labels.instance }} failed.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-httpprobefailed"
          },
          "expr": "probe_success != 1",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "HttpProbeSlow",
          "annotations": {
            "message": "{{ $labels.instance }} takes {{ $value }} seconds to respond.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-httpprobeslow"
          },
          "expr": "sum by (instance) (probe_http_duration_seconds) > 3",
          "for": "15m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the target system's resource usage for anomalias.",
              "Check if the target application has been recently rescheduled and is still settling."
            ]
          }
        },
        {
          "alert": "HttpCertExpiresSoon",
          "annotations": {
            "message": "The certificate for {{ $labels.instance }} expires in less than 3 days.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-httpcertexpiressoon"
          },
          "expr": "probe_ssl_earliest_cert_expiry - time() < 3*24*3600",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "HttpCertExpiresVerySoon",
          "annotations": {
            "message": "The certificate for {{ $labels.instance }} expires in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-httpcertexpiresverysoon"
          },
          "expr": "probe_ssl_earliest_cert_expiry - time() < 24*3600",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "cadvisor",
      "rules": [
        {
          "alert": "CadvisorDown",
          "annotations": {
            "message": "Cadvisor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-cadvisordown"
          },
          "expr": "absent(up{job=\"cadvisor\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "cert-manager",
      "rules": [
        {
          "alert": "CertManagerCertExpiresSoon",
          "annotations": {
            "message": "The certificate {{ $labels.name }} expires in less than 3 days.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-certmanagercertexpiressoon"
          },
          "expr": "certmanager_certificate_expiration_timestamp_seconds - time() < 3*24*3600",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "CertManagerCertExpiresVerySoon",
          "annotations": {
            "message": "The certificate {{ $labels.name }} expires in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-certmanagercertexpiresverysoon"
          },
          "expr": "certmanager_certificate_expiration_timestamp_seconds - time() < 24*3600",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "elasticsearch",
      "rules": [
        {
          "alert": "ElasticsearchHeapTooHigh",
          "annotations": {
            "message": "The heap usage of Elasticsearch node {{ $labels.name }} is over 90%.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-elasticsearchheaptoohigh"
          },
          "expr": "elasticsearch_jvm_memory_used_bytes{area=\"heap\"} / elasticsearch_jvm_memory_max_bytes{area=\"heap\"} > 0.9",
          "for": "15m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the pod's logs for anomalities.",
              "If it is a data node, check the shard allocation via `http://es-data:9200/_cat/shards?v`."
            ]
          }
        },
        {
          "alert": "ElasticsearchClusterUnavailable",
          "annotations": {
            "message": "The Elasticsearch cluster health endpoint does not respond to scrapes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-elasticsearchclusterunavailable"
          },
          "expr": "elasticsearch_cluster_health_up == 0",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "ElasticsearchClusterUnhealthy",
          "annotations": {
            "message": "The Elasticsearch cluster is not healthy.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-elasticsearchclusterunhealthy"
          },
          "expr": "elasticsearch_cluster_health_status{color=\"green\"} == 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "ElasticsearchUnassignedShards",
          "annotations": {
            "message": "There are {{ $value }} unassigned shards in the Elasticsearch cluster.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-elasticsearchunassignedshards"
          },
          "expr": "elasticsearch_cluster_health_unassigned_shards > 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check the shard allocation via `http://es-data:9200/_cat/shards?v`."
            ]
          }
        }
      ]
    },
    {
      "name": "fluentbit",
      "rules": [
        {
          "alert": "FluentbitManyFailedRetries",
          "annotations": {
            "message": "Fluentbit pod `{{ $labels.pod }}` on `{{ $labels.node }}` is experiencing an elevated failed retry rate.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-fluentbitmanyfailedretries"
          },
          "expr": "sum by (namespace, pod, node) (kube_pod_info) *\n  on (namespace, pod)\n  group_right (node)\n  rate(fluentbit_output_retries_failed_total[1m]) > 0\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Ensure the target Elasticsearch cluster is healthy and accepts new documents (in certain conditions Elasticsearch clusters become read-only).",
              "Ensure that `Retry_Limit` is not set to `False` (infinite) to prevent unprocessable logs from stopping the ingestion of new logs."
            ]
          }
        },
        {
          "alert": "FluentbitManyOutputErrors",
          "annotations": {
            "message": "Fluentbit pod `{{ $labels.pod }}` on `{{ $labels.node }}` is experiencing an elevated output error rate.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-fluentbitmanyoutputerrors"
          },
          "expr": "sum by (namespace, pod, node) (kube_pod_info) *\n  on (namespace, pod)\n  group_right (node)\n  rate(fluentbit_output_errors_total[1m]) > 0\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Ensure the target Elasticsearch cluster is healthy and accepts new documents (in certain conditions Elasticsearch clusters become read-only).",
              "Ensure that `Retry_Limit` is not set to `False` (infinite) to prevent unprocessable logs from stopping the ingestion of new logs."
            ]
          }
        },
        {
          "alert": "FluentbitNotProcessingNewLogs",
          "annotations": {
            "message": "Fluentbit pod `{{ $labels.pod }}` on `{{ $labels.node }}` has not processed any new logs for the last 30 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-fluentbitnotprocessingnewlogs"
          },
          "expr": "sum by (namespace, pod, node) (kube_pod_info) *\n  on (namespace, pod)\n  group_right (node)\n  rate(fluentbit_output_proc_records_total[1m]) == 0\n",
          "for": "30m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check if there are no other log-generating pods running on the same node."
            ]
          }
        }
      ]
    },
    {
      "name": "helm-exporter",
      "rules": [
        {
          "alert": "HelmReleaseNotDeployed",
          "annotations": {
            "message": "The Helm release `{{ $labels.release }}` (`{{ $labels.chart }}` chart in namespace `{{ $labels.exported_namespace }}`) in version {{ $labels.version }} has not been ready for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-helmreleasenotdeployed"
          },
          "expr": "helm_chart_info != 1",
          "for": "15m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the installed Helm releases via `helm --tiller-namespace kubermtic-installer ls`.",
              "If all releases are status `DEPLOYED`, make sure the helme-exporter is looking at the correct Tiller by checking the `values.yaml` flag `helmExporter.tillerNamespace`.",
              "If Helm cannot repair the chart automatically, delete/purge the chart (`helm delete --purge [RELEASE]`) and re-install the chart again. Re-installing charts will not affect any existing data in existing PersistentVolumeClaims."
            ]
          }
        }
      ]
    },
    {
      "name": "kube-apiserver",
      "rules": [
        {
          "alert": "KubernetesApiserverDown",
          "annotations": {
            "message": "KubernetesApiserver has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubernetesapiserverdown"
          },
          "expr": "absent(up{job=\"apiserver\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeAPILatencyHigh",
          "annotations": {
            "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
          },
          "expr": "cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} > 1",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeAPILatencyHigh",
          "annotations": {
            "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
          },
          "expr": "cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} > 4",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeAPIErrorsHigh",
          "annotations": {
            "message": "API server is returning errors for {{ $value }}% of requests.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
          },
          "expr": "sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) without(instance, pod)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) without(instance, pod) * 100 > 10\n",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeAPIErrorsHigh",
          "annotations": {
            "message": "API server is returning errors for {{ $value }}% of requests.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
          },
          "expr": "sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) without(instance, pod)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) without(instance, pod) * 100 > 5\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "A client certificate used to authenticate to the apiserver is expiring in less than 7 days.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0\nand\nhistogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800\n",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) on how to renew certificates.",
              "If your certificate has already expired, the steps in the documentation might not work. Check [Github](https://github.com/kubernetes/kubeadm/issues/581#issuecomment-421477139) for hints about fixing your cluster."
            ]
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "A client certificate used to authenticate to the apiserver is expiring in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0\nand\nhistogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400\n",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Urgently renew your certificates. Expired certificates can make fixing the cluster difficult to begin with.",
              "Check the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) on how to renew certificates.",
              "If your certificate has already expired, the steps in the documentation might not work. Check [Github](https://github.com/kubernetes/kubeadm/issues/581#issuecomment-421477139) for hints about fixing your cluster."
            ]
          }
        }
      ]
    },
    {
      "name": "kube-kubelet",
      "rules": [
        {
          "alert": "KubeletDown",
          "annotations": {
            "message": "Kubelet has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletdown"
          },
          "expr": "absent(up{job=\"kubelet\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubePersistentVolumeUsageCritical",
          "annotations": {
            "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is only {{ printf \"%0.0f\" $value }}% free.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumeusagecritical"
          },
          "expr": "100 * kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n  < 3\n",
          "for": "1m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubePersistentVolumeFullInFourDays",
          "annotations": {
            "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value }} bytes are available.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumefullinfourdays"
          },
          "expr": "(\n  kubelet_volume_stats_used_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) > 0.85\nand\npredict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\n",
          "for": "5m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeletTooManyPods",
          "annotations": {
            "message": "Kubelet {{ $labels.instance }} is running {{ $value }} pods, close to the limit of 110.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubelettoomanypods"
          },
          "expr": "kubelet_running_pod_count{job=\"kubelet\"} > 110 * 0.9",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientErrors",
          "annotations": {
            "message": "The kubelet on {{ $labels.instance }} is experiencing {{ printf \"%0.0f\" $value }}% errors.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"(5..|<error>)\",job=\"kubelet\"}[5m])) by (instance)\n  /\nsum(rate(rest_client_requests_total{job=\"kubelet\"}[5m])) by (instance))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientErrors",
          "annotations": {
            "message": "The pod {{ $labels.namespace }}/{{ $labels.pod }} is experiencing {{ printf \"%0.0f\" $value }}% errors.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"(5..|<error>)\",job=\"pods\"}[5m])) by (namespace, pod)\n  /\nsum(rate(rest_client_requests_total{job=\"pods\"}[5m])) by (namespace, pod))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeletRuntimeErrors",
          "annotations": {
            "message": "The kubelet on {{ $labels.instance }} is having an elevated error rate for container runtime oprations.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletruntimeerrors"
          },
          "expr": "sum(rate(kubelet_runtime_operations_errors{job=\"kubelet\"}[5m])) by (instance) > 0.1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeletCGroupManagerLatencyHigh",
          "annotations": {
            "message": "The kubelet's cgroup manager latency on {{ $labels.instance }} has been elevated ({{ printf \"%0.2f\" $value }}ms) for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletcgroupmanagerlatencyhigh"
          },
          "expr": "sum(rate(kubelet_cgroup_manager_latency_microseconds{quantile=\"0.9\"}[5m])) by (instance) / 1000 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeletPodWorkerLatencyHigh",
          "annotations": {
            "message": "The kubelet's pod worker latency for {{ $labels.operation_type }} operations on {{ $labels.instance }} has been elevated ({{ printf \"%0.2f\" $value }}ms) for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletpodworkerlatencyhigh"
          },
          "expr": "sum(rate(kubelet_pod_worker_latency_microseconds{quantile=\"0.9\"}[5m])) by (instance, operation_type) / 1000 > 250\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeVersionMismatch",
          "annotations": {
            "message": "There are {{ $value }} different versions of Kubernetes components running.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeversionmismatch"
          },
          "expr": "count(count(kubernetes_build_info{job!=\"dns\"}) by (gitVersion)) > 1",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "kube-state-metrics",
      "rules": [
        {
          "alert": "KubeStateMetricsDown",
          "annotations": {
            "message": "KubeStateMetrics has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatemetricsdown"
          },
          "expr": "absent(up{job=\"kube-state-metrics\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubePodCrashLooping",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodcrashlooping"
          },
          "expr": "rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[15m]) * 60 * 5 > 0",
          "for": "1h",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check the pod's logs."
            ]
          }
        },
        {
          "alert": "KubePodNotReady",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodnotready"
          },
          "expr": "sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}) > 0",
          "for": "30m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check the pod via `kubectl describe pod [POD]` to find out about scheduling issues."
            ]
          }
        },
        {
          "alert": "KubeDeploymentGenerationMismatch",
          "annotations": {
            "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentgenerationmismatch"
          },
          "expr": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDeploymentReplicasMismatch",
          "annotations": {
            "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than an hour.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentreplicasmismatch"
          },
          "expr": "kube_deployment_spec_replicas{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_status_replicas_available{job=\"kube-state-metrics\"}\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetReplicasMismatch",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetreplicasmismatch"
          },
          "expr": "kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_status_replicas{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetGenerationMismatch",
          "annotations": {
            "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetgenerationmismatch"
          },
          "expr": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetUpdateNotRolledOut",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetupdatenotrolledout"
          },
          "expr": "max without (revision) (\n  kube_statefulset_status_current_revision{job=\"kube-state-metrics\"}\n    unless\n  kube_statefulset_status_update_revision{job=\"kube-state-metrics\"}\n)\n  *\n(\n  kube_statefulset_replicas{job=\"kube-state-metrics\"}\n    !=\n  kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"}\n)\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDaemonSetRolloutStuck",
          "annotations": {
            "message": "Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetrolloutstuck"
          },
          "expr": "kube_daemonset_status_number_ready{job=\"kube-state-metrics\"}\n  /\nkube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"} * 100 < 100\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDaemonSetNotScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetnotscheduled"
          },
          "expr": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"}\n  -\nkube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"} > 0\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeDaemonSetMisScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetmisscheduled"
          },
          "expr": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"} > 0",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeCronJobRunning",
          "annotations": {
            "message": "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecronjobrunning"
          },
          "expr": "time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\"} > 3600",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeJobCompletion",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobcompletion"
          },
          "expr": "kube_job_spec_completions{job=\"kube-state-metrics\"} - kube_job_status_succeeded{job=\"kube-state-metrics\"} > 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeJobFailed",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobfailed"
          },
          "expr": "kube_job_status_failed{job=\"kube-state-metrics\"} > 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.cpu\"})\n  /\nsum(node:node_num_cpu:sum)\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for pods and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n  /\nsum(node:node_num_cpu:sum)\n  >\n(count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.memory\"})\n  /\nsum(node_memory_MemTotal_bytes{app=\"node-exporter\"})\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for pods and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n  /\nsum(node_memory_MemTotal_bytes)\n  >\n(count(node:node_num_cpu:sum)-1)\n  /\ncount(node:node_num_cpu:sum)\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeQuotaExceeded",
          "annotations": {
            "message": "Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\" $value }}% of its {{ $labels.resource }} quota.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubequotaexceeded"
          },
          "expr": "100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\"} > 0)\n  > 90\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubePodOOMKilled",
          "annotations": {
            "message": "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 30 minutes.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodoomkilled"
          },
          "expr": "(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 30m >= 2)\nand\nignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[30m]) == 1\n",
          "for": "0m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeNodeNotReady",
          "annotations": {
            "message": "{{ $labels.node }} has been unready for more than an hour.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubenodenotready"
          },
          "expr": "kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "node-exporter",
      "rules": [
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemspacefillingup"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 4 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemspacefillingup"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemoutofspace"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemoutofspace"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 3\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemfilesfillingup"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 4 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemfilesfillingup"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfFiles",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available inodes left.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemoutoffiles"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodefilesystemoutofspace"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 3\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeNetworkReceiveErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while receiving packets ({{ $value }} errors in two minutes).",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodenetworkreceiveerrs"
          },
          "expr": "increase(node_network_receive_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeNetworkTransmitErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while transmitting packets ({{ $value }} errors in two minutes).",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-nodenetworktransmiterrs"
          },
          "expr": "increase(node_network_transmit_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "prometheus",
      "rules": [
        {
          "alert": "PromScrapeFailed",
          "annotations": {
            "message": "Prometheus failed to scrape a target {{ $labels.job }} / {{ $labels.instance }}.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-promscrapefailed"
          },
          "expr": "up != 1",
          "for": "15m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable."
            ]
          }
        },
        {
          "alert": "PromBadConfig",
          "annotations": {
            "mesage": "Prometheus failed to reload config.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-prombadconfig"
          },
          "expr": "prometheus_config_last_reload_successful{job=\"prometheus\"} == 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Check the `prometheus-rules` configmap via `kubectl -n monitoring get configmap prometheus-rules -o yaml`."
            ]
          }
        },
        {
          "alert": "PromAlertmanagerBadConfig",
          "annotations": {
            "message": "Alertmanager failed to reload config.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-promalertmanagerbadconfig"
          },
          "expr": "alertmanager_config_last_reload_successful{job=\"alertmanager\"} == 0",
          "for": "10m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check Alertmanager pod's logs via `kubectl -n monitoring logs alertmanager-0`, `-1` and `-2`.",
              "Check the `alertmanager` secret via `kubectl -n monitoring get secret alertmanager -o yaml`."
            ]
          }
        },
        {
          "alert": "PromAlertsFailed",
          "annotations": {
            "message": "Alertmanager failed to send an alert.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-promalertsfailed"
          },
          "expr": "sum(increase(alertmanager_notifications_failed_total{job=\"alertmanager\"}[5m])) by (namespace) > 0",
          "for": "5m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Make sure the Alertmanager StatefulSet is running: `kubectl -n monitoring get pods`."
            ]
          }
        },
        {
          "alert": "PromRemoteStorageFailures",
          "annotations": {
            "message": "Prometheus failed to send {{ printf \"%.1f\" $value }}% samples.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-promremotestoragefailures"
          },
          "expr": "(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) * 100)\n  /\n(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus\"}[1m]))\n  > 1\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Ensure that the Prometheus volume has not reached capacity.",
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`."
            ]
          }
        },
        {
          "alert": "PromRuleFailures",
          "annotations": {
            "message": "Prometheus failed to evaluate {{ printf \"%.1f\" $value }} rules/sec.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-promrulefailures"
          },
          "expr": "rate(prometheus_rule_evaluation_failures_total{job=\"prometheus\"}[1m]) > 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Check CPU/memory pressure on the node."
            ]
          }
        }
      ]
    },
    {
      "name": "thanos",
      "rules": [
        {
          "alert": "ThanosSidecarDown",
          "annotations": {
            "message": "The Thanos sidecar in `{{ $labels.namespace }}/{{ $labels.pod }}` is down.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanossidecardown"
          },
          "expr": "thanos_sidecar_prometheus_up != 1",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "ThanosSidecarNoHeartbeat",
          "annotations": {
            "message": "The Thanos sidecar in `{{ $labels.namespace }}/{{ $labels.pod }}` didn't send a heartbeat in {{ $value }} seconds.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanossidecardown"
          },
          "expr": "time() - thanos_sidecar_last_heartbeat_success_time_seconds > 60",
          "for": "3m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "ThanosCompactorManyRetries",
          "annotations": {
            "message": "The Thanos compactor in `{{ $labels.namespace }}` is experiencing a high retry rate.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanoscompactormanyretries"
          },
          "expr": "sum(rate(thanos_compactor_retries_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the `thanos-compact` pod's logs."
            ]
          }
        },
        {
          "alert": "ThanosShipperManyDirSyncFailures",
          "annotations": {
            "message": "The Thanos shipper in `{{ $labels.namespace }}/{{ $labels.pod }}` is experiencing a high dir-sync failure rate.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanosshippermanydirsyncfailures"
          },
          "expr": "sum(rate(thanos_shipper_dir_sync_failures_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the `thanos` containers's logs inside the Prometheus pod."
            ]
          }
        },
        {
          "alert": "ThanosManyPanicRecoveries",
          "annotations": {
            "message": "The Thanos component in `{{ $labels.namespace }}/{{ $labels.pod }}` is experiencing a panic recovery rate.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanosmanypanicrecoveries"
          },
          "expr": "sum(rate(thanos_grpc_req_panics_recovered_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "ThanosManyBlockLoadFailures",
          "annotations": {
            "message": "The Thanos store in `{{ $labels.namespace }}/{{ $labels.pod }}` is experiencing a many failed block loads.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanosmanyblockloadfailures"
          },
          "expr": "sum(rate(thanos_bucket_store_block_load_failures_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "ThanosManyBlockDropFailures",
          "annotations": {
            "message": "The Thanos store in `{{ $labels.namespace }}/{{ $labels.pod }}` is experiencing a many failed block drops.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-thanosmanyblockdropfailures"
          },
          "expr": "sum(rate(thanos_bucket_store_block_drop_failures_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "velero",
      "rules": [
        {
          "alert": "VeleroBackupTakesTooLong",
          "annotations": {
            "message": "Backup schedule {{ $labels.schedule }} has been taking more than 60min already.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-velerobackuptakestoolong"
          },
          "expr": "(velero_backup_attempt_total - velero_backup_success_total) > 0",
          "for": "60m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check if a backup is really in \"InProgress\" state via `velero -n velero backup get`.",
              "Check the backup logs via `velero -n velero backup logs [BACKUP_NAME]`.",
              "Depending on the backup, find the pod and check the processes inside that pod or any sidecar containers."
            ]
          }
        },
        {
          "alert": "VeleroNoRecentBackup",
          "annotations": {
            "message": "There has not been a successful backup for schedule {{ $labels.schedule }} in the last 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-veleronorecentbackup"
          },
          "expr": "time() - velero_backup_last_successful_timestamp{schedule!=\"\"} > 3600*25",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check if really no backups happened via `velero -n velero backup get`.",
              "If a backup failed, check its logs via `velero -n velero backup logs [BACKUP_NAME]`.",
              "If a backup was not even triggered, check the Velero server's logs via `kubectl -n velero logs -l 'name=velero-server'`.",
              "Make sure the Velero server pod has not been rescheduled and possibly opt to schedule it on a stable node using a node affinity."
            ]
          }
        }
      ]
    },
    {
      "name": "kubermatic",
      "rules": [
        {
          "alert": "KubermaticAPIDown",
          "annotations": {
            "message": "KubermaticAPI has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticapidown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",role=\"kubermatic-api\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable.",
              "Ensure that the API pod's logs and that it is not crashlooping."
            ]
          }
        },
        {
          "alert": "KubermaticAPITooManyErrors",
          "annotations": {
            "message": "Kubermatic API is returning a high rate of HTTP 5xx responses.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticapitoomanyerrors"
          },
          "expr": "sum(rate(http_requests_total{role=\"kubermatic-api\",code=~\"5..\"}[5m])) > 0.1",
          "for": "15m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the API pod's logs."
            ]
          }
        },
        {
          "alert": "KubermaticAPITooManyInitNodeDeloymentFailures",
          "annotations": {
            "message": "Kubermatic API is failing to create too many initial node deployments."
          },
          "expr": "sum(rate(kubermatic_api_init_node_deployment_failures[5m])) > 0.01",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "kubermatic",
      "rules": [
        {
          "alert": "KubermaticTooManyUnhandledErrors",
          "annotations": {
            "message": "Kubermatic controller manager in {{ $labels.namespace }} is experiencing too many errors.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermatictoomanyunhandlederrors"
          },
          "expr": "sum(rate(kubermatic_controller_manager_unhandled_errors_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the controller-manager pod's logs."
            ]
          }
        },
        {
          "alert": "KubermaticClusterDeletionTakesTooLong",
          "annotations": {
            "message": "Cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticclusterdeletiontakestoolong"
          },
          "expr": "(time() - max by (cluster) (kubermatic_cluster_deleted)) > 30*60",
          "for": "0m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the machine-controller's logs via `kubectl -n cluster-XYZ logs -l 'app=machine-controller'` for errors related to cloud provider integrations. Expired credentials or manually deleted cloud provider resources are common reasons for failing deletions.",
              "Check the cluster's status itself via `kubectl describe cluster XYZ`.",
              "If all resources have been cleaned up, remove the blocking finalizer (e.g. `kubermatic.io/delete-nodes`) from the cluster resource.",
              "If nothing else helps, manually delete the cluster namespace as a last resort."
            ]
          }
        },
        {
          "alert": "KubermaticAddonDeletionTakesTooLong",
          "annotations": {
            "message": "Addon {{ $labels.addon }} in cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticaddondeletiontakestoolong"
          },
          "expr": "(time() - max by (cluster,addon) (kubermatic_addon_deleted)) > 30*60",
          "for": "0m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check the kubermatic controller-manager's logs via `kubectl -n kubermatic logs -l 'role=controller-manager'` for errors related to deletion of the addon. Manually deleted resources inside of the user cluster is a common reason for failing deletions.",
              "If all resources of the addon inside the user cluster have been cleaned up, remove the blocking finalizer (e.g. `cleanup-manifests`) from the addon resource."
            ]
          }
        },
        {
          "alert": "KubermaticControllerManagerDown",
          "annotations": {
            "message": "KubermaticControllerManager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticcontrollermanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",role=\"controller-manager\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable.",
              "Ensure that the controller-manager pod's logs and that it is not crashlooping."
            ]
          }
        },
        {
          "alert": "OpenVPNServerDown",
          "annotations": {
            "message": "There is no healthy OpenVPN server in cluster {{ $labels.cluster }}.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-openvpnserverdown"
          },
          "expr": "absent(kube_deployment_status_replicas_available{deployment=\"openvpn-server\",cluster!=\"\"} > 0)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "UserClusterPrometheusAbsent",
          "annotations": {
            "message": "There is no Prometheus in cluster {{ $labels.name }}.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-userclusterprometheusdisappeared"
          },
          "expr": "(\n  kubermatic_cluster_info * on (name) group_left\n  label_replace(up{job=\"clusters\"}, \"name\", \"$1\", \"namespace\", \"cluster-(.+)\")\n  or\n  kubermatic_cluster_info * 0\n) == 0\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubermaticClusterPaused",
          "annotations": {
            "message": "Cluster {{ $labels.name }} has been paused and will not be reconciled until the pause flag is reset."
          },
          "expr": "label_replace(kubermatic_cluster_info{pause=\"true\"}, \"cluster\", \"$0\", \"name\", \".+\")",
          "labels": {
            "severity": "none"
          }
        }
      ]
    },
    {
      "name": "kube-controller-manager",
      "rules": [
        {
          "alert": "KubeControllerManagerDown",
          "annotations": {
            "message": "No healthy controller-manager pods exist inside the cluster.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecontrollermanagerdown"
          },
          "expr": "absent(:ready_kube_controller_managers:sum) or :ready_kube_controller_managers:sum == 0",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "kube-scheduler",
      "rules": [
        {
          "alert": "KubeSchedulerDown",
          "annotations": {
            "message": "No healthy scheduler pods exist inside the cluster.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeschedulerdown"
          },
          "expr": "absent(:ready_kube_schedulers:sum) or :ready_kube_schedulers:sum == 0",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    }
  ]
}
