{
  "compiled": "2019-01-28T09:52:22Z",
  "groups": [
    {
      "name": "ark",
      "rules": [
        {
          "alert": "ArkBackupTakesTooLong",
          "annotations": {
            "message": "Backup schedule {{ $labels.schedule }} has been taking more than 60min already.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-arkbackuptakestoolong"
          },
          "expr": "(ark_backup_attempt_total - ark_backup_success_total) > 0",
          "for": "60m",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check if a backup is really in \"InProgress\" state via `ark backup get`.",
              "Check the backup logs via `ark -n ark backup logs [BACKUP_NAME]`.",
              "Depending on the backup, find the pod and check the processes inside that pod or any sidecar containers."
            ]
          }
        },
        {
          "alert": "ArkNoRecentBackup",
          "annotations": {
            "message": "There has not been a successful backup for schedule {{ $labels.schedule }} in the last 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-arknorecentbackup"
          },
          "expr": "changes(ark_backup_success_total{schedule!=\"\"}[25h]) < 1",
          "labels": {
            "severity": "warning"
          },
          "runbook": {
            "steps": [
              "Check if really no backups happened via `ark backup get`.",
              "If a backup failed, check its logs via `ark -n ark backup logs [BACKUP_NAME]`.",
              "If a backup was not even triggered, check the Ark server's logs via `kubectl -n ark logs -l 'name=ark-server'`."
            ]
          }
        }
      ]
    },
    {
      "name": "kubermatic",
      "rules": [
        {
          "alert": "KubermaticTooManyUnhandledErrors",
          "annotations": {
            "message": "Kubermatic controller manager in {{ $labels.namespace }} is experiencing too many errors.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermatictoomanyunhandlederrors"
          },
          "expr": "sum(rate(kubermatic_controller_manager_unhandled_errors_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubermaticClusterDeletionTakesTooLong",
          "annotations": {
            "message": "Cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticclusterdeletiontakestoolong"
          },
          "expr": "(time() - max by (cluster) (kubermatic_cluster_deleted)) > 30*60",
          "for": "0m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubermaticAPIDown",
          "annotations": {
            "message": "KubermaticAPI has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticapidown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",role=\"kubermatic-api\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubermaticControllerManagerDown",
          "annotations": {
            "message": "KubermaticControllerManager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubermaticcontrollermanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",role=\"controller-manager\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "kubernetes-absent",
      "rules": [
        {
          "alert": "CadvisorDown",
          "annotations": {
            "message": "Cadvisor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-cadvisordown"
          },
          "expr": "absent(up{job=\"cadvisor\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubernetesApiserverDown",
          "annotations": {
            "message": "KubernetesApiserver has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubernetesapiserverdown"
          },
          "expr": "absent(up{job=\"apiserver\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeControllerManagerDown",
          "annotations": {
            "message": "KubeControllerManager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecontrollermanagerdown"
          },
          "expr": "absent(up{job=\"controller-manager\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeSchedulerDown",
          "annotations": {
            "message": "KubeScheduler has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeschedulerdown"
          },
          "expr": "absent(up{job=\"scheduler\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStateMetricsDown",
          "annotations": {
            "message": "KubeStateMetrics has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatemetricsdown"
          },
          "expr": "absent(up{job=\"kube-state-metrics\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeletDown",
          "annotations": {
            "message": "Kubelet has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeletdown"
          },
          "expr": "absent(up{job=\"kubelet\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "kubernetes-apps",
      "rules": [
        {
          "alert": "KubePodCrashLooping",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodcrashlooping"
          },
          "expr": "rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[15m]) * 60 * 5 > 0",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubePodNotReady",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepodnotready"
          },
          "expr": "sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}) > 0",
          "for": "30m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDeploymentGenerationMismatch",
          "annotations": {
            "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentgenerationmismatch"
          },
          "expr": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDeploymentReplicasMismatch",
          "annotations": {
            "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than an hour.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedeploymentreplicasmismatch"
          },
          "expr": "kube_deployment_spec_replicas{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_status_replicas_available{job=\"kube-state-metrics\"}\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetReplicasMismatch",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetreplicasmismatch"
          },
          "expr": "kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_status_replicas{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetGenerationMismatch",
          "annotations": {
            "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetgenerationmismatch"
          },
          "expr": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeStatefulSetUpdateNotRolledOut",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubestatefulsetupdatenotrolledout"
          },
          "expr": "max without (revision) (\n  kube_statefulset_status_current_revision{job=\"kube-state-metrics\"}\n    unless\n  kube_statefulset_status_update_revision{job=\"kube-state-metrics\"}\n)\n  *\n(\n  kube_statefulset_replicas{job=\"kube-state-metrics\"}\n    !=\n  kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"}\n)\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDaemonSetRolloutStuck",
          "annotations": {
            "message": "Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetrolloutstuck"
          },
          "expr": "kube_daemonset_status_number_ready{job=\"kube-state-metrics\"}\n  /\nkube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"} * 100 < 100\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeDaemonSetNotScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetnotscheduled"
          },
          "expr": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"}\n  -\nkube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"} > 0\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeDaemonSetMisScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubedaemonsetmisscheduled"
          },
          "expr": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"} > 0",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeCronJobRunning",
          "annotations": {
            "message": "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecronjobrunning"
          },
          "expr": "time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\"} > 3600",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeJobCompletion",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobcompletion"
          },
          "expr": "kube_job_spec_completions{job=\"kube-state-metrics\"} - kube_job_status_succeeded{job=\"kube-state-metrics\"} > 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeJobFailed",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubejobfailed"
          },
          "expr": "kube_job_status_failed{job=\"kube-state-metrics\"} > 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "kubernetes-resources",
      "rules": [
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for pods and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n  /\nsum(node:node_num_cpu:sum)\n  >\n(count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for pods and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n  /\nsum(node_memory_MemTotal_bytes)\n  >\n(count(node:node_num_cpu:sum)-1)\n  /\ncount(node:node_num_cpu:sum)\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.cpu\"})\n  /\nsum(node:node_num_cpu:sum)\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubememovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.memory\"})\n  /\nsum(node_memory_MemTotal_bytes{app=\"node-exporter\"})\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeQuotaExceeded",
          "annotations": {
            "message": "Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\" $value }}% of its {{ $labels.resource }} quota.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubequotaexceeded"
          },
          "expr": "100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\"} > 0)\n  > 90\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubePodOOMKilled",
          "annotations": {
            "message": "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 30 minutes."
          },
          "expr": "(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 30m >= 2)\nand\nignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[30m]) == 1\n",
          "for": "0m",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "kubernetes-storage",
      "rules": [
        {
          "alert": "KubePersistentVolumeUsageCritical",
          "annotations": {
            "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is only {{ printf \"%0.0f\" $value }}% free.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumeusagecritical"
          },
          "expr": "100 * kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n  < 3\n",
          "for": "1m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubePersistentVolumeFullInFourDays",
          "annotations": {
            "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value }} bytes are available.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubepersistentvolumefullinfourdays"
          },
          "expr": "(\n  kubelet_volume_stats_used_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) > 0.85\nand\npredict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\n",
          "for": "5m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "kubernetes-system",
      "rules": [
        {
          "alert": "KubeNodeNotReady",
          "annotations": {
            "message": "{{ $labels.node }} has been unready for more than an hour.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubenodenotready"
          },
          "expr": "kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeVersionMismatch",
          "annotations": {
            "message": "There are {{ $value }} different versions of Kubernetes components running.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeversionmismatch"
          },
          "expr": "count(count(kubernetes_build_info{job!=\"dns\"}) by (gitVersion)) > 1",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientErrors",
          "annotations": {
            "message": "The pod {{ $labels.namespace }}/{{ $labels.pod }} is experiencing {{ printf \"%0.0f\" $value }}% errors.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"5..\",job=\"pods\"}[5m])) by (namespace, pod)\n  /\nsum(rate(rest_client_requests_total{job=\"pods\"}[5m])) by (namespace, pod))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientErrors",
          "annotations": {
            "message": "The kubelet on {{ $labels.instance }} is experiencing {{ printf \"%0.0f\" $value }}% errors.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"5..\",job=\"kubelet\"}[5m])) by (instance)\n  /\nsum(rate(rest_client_requests_total{job=\"kubelet\"}[5m])) by (instance))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeletTooManyPods",
          "annotations": {
            "message": "Kubelet {{ $labels.instance }} is running {{ $value }} pods, close to the limit of 110.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubelettoomanypods"
          },
          "expr": "kubelet_running_pod_count{job=\"kubelet\"} > 110 * 0.9",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeAPILatencyHigh",
          "annotations": {
            "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
          },
          "expr": "cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} > 1",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeAPILatencyHigh",
          "annotations": {
            "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapilatencyhigh"
          },
          "expr": "cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$\"} > 4",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeAPIErrorsHigh",
          "annotations": {
            "message": "API server is returning errors for {{ $value }}% of requests.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
          },
          "expr": "sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) without(instance, pod)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) without(instance, pod) * 100 > 10\n",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "KubeAPIErrorsHigh",
          "annotations": {
            "message": "API server is returning errors for {{ $value }}% of requests.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeapierrorshigh"
          },
          "expr": "sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m])) without(instance, pod)\n  /\nsum(rate(apiserver_request_count{job=\"apiserver\"}[5m])) without(instance, pod) * 100 > 5\n",
          "for": "10m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "Kubernetes API certificate is expiring in less than 7 days.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800\n",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "Kubernetes API certificate is expiring in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.io/monitoring/runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400\n",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "node-exporter",
      "rules": [
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 24 hours.\n"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 4 hours.\n"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.\n"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.\n"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 3\nand\nnode_filesystem_readonly_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 24 hours.\n"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 4 hours.\n"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfFiles",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available inodes left.\n"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available space left.\n"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 3\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeNetworkReceiveErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while receiving packets ({{ $value }} errors in two minutes).\n"
          },
          "expr": "increase(node_network_receive_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "NodeNetworkTransmitErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while transmitting packets ({{ $value }} errors in two minutes).\n"
          },
          "expr": "increase(node_network_transmit_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    },
    {
      "name": "prometheus",
      "rules": [
        {
          "alert": "PromScrapeFailed",
          "annotations": {
            "message": "Prometheus failed to scrape a target {{ $labels.job }} / {{ $labels.instance }}."
          },
          "expr": "up != 1",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "PromBadConfig",
          "annotations": {
            "mesage": "Prometheus failed to reload config, see container logs."
          },
          "expr": "prometheus_config_last_reload_successful{job=\"prometheus\"} == 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "PromAlertmanagerBadConfig",
          "annotations": {
            "message": "Alertmanager failed to reload config, see container logs."
          },
          "expr": "alertmanager_config_last_reload_successful{job=\"alertmanager\"} == 0",
          "for": "10m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "PromAlertsFailed",
          "annotations": {
            "message": "Alertmanager failed to send an alert."
          },
          "expr": "sum(increase(alertmanager_notifications_failed_total{job=\"alertmanager\"}[5m])) by (namespace) > 0",
          "for": "5m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "PromRemoteStorageFailures",
          "annotations": {
            "message": "Prometheus failed to send {{ printf \"%.1f\" $value }}% samples."
          },
          "expr": "(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) * 100)\n  /\n(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus\"}[1m]))\n  > 1\n",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        },
        {
          "alert": "PromRuleFailures",
          "annotations": {
            "message": "Prometheus failed to evaluate {{ printf \"%.1f\" $value }} rules/sec."
          },
          "expr": "rate(prometheus_rule_evaluation_failures_total{job=\"prometheus\"}[1m]) > 0",
          "for": "15m",
          "labels": {
            "severity": "critical"
          }
        }
      ]
    }
  ]
}
