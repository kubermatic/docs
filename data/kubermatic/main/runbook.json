{
  "groups": [
    {
      "name": "blackbox-exporter",
      "rules": [
        {
          "alert": "HttpProbeFailed",
          "annotations": {
            "message": "Probing the blackbox-exporter target {{ $labels.instance }} failed.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-httpprobefailed"
          },
          "expr": "probe_success != 1",
          "for": "5m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "blackbox-exporter"
          }
        },
        {
          "alert": "HttpProbeSlow",
          "annotations": {
            "message": "{{ $labels.instance }} takes {{ $value }} seconds to respond.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-httpprobeslow"
          },
          "expr": "sum by (instance) (probe_http_duration_seconds) > 3",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "blackbox-exporter"
          },
          "runbook": {
            "steps": [
              "Check the target system's resource usage for anomalias.",
              "Check if the target application has been recently rescheduled and is still settling."
            ]
          }
        },
        {
          "alert": "HttpCertExpiresSoon",
          "annotations": {
            "message": "The certificate for {{ $labels.instance }} expires in less than 3 days.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-httpcertexpiressoon"
          },
          "expr": "probe_ssl_earliest_cert_expiry - time() < 3*24*3600",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "blackbox-exporter"
          }
        },
        {
          "alert": "HttpCertExpiresVerySoon",
          "annotations": {
            "message": "The certificate for {{ $labels.instance }} expires in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-httpcertexpiresverysoon"
          },
          "expr": "probe_ssl_earliest_cert_expiry - time() < 24*3600",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }}",
            "service": "blackbox-exporter"
          }
        }
      ]
    },
    {
      "name": "cert-manager",
      "rules": [
        {
          "alert": "CertManagerCertExpiresSoon",
          "annotations": {
            "message": "The certificate {{ $labels.name }} expires in less than 3 days.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-certmanagercertexpiressoon"
          },
          "expr": "certmanager_certificate_expiration_timestamp_seconds - time() < 3*24*3600",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.name }}",
            "service": "cert-manager"
          }
        },
        {
          "alert": "CertManagerCertExpiresVerySoon",
          "annotations": {
            "message": "The certificate {{ $labels.name }} expires in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-certmanagercertexpiresverysoon"
          },
          "expr": "certmanager_certificate_expiration_timestamp_seconds - time() < 24*3600",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.name }}",
            "service": "cert-manager"
          }
        }
      ]
    },
    {
      "name": "helm-exporter",
      "rules": [
        {
          "alert": "HelmReleaseNotDeployed",
          "annotations": {
            "message": "The Helm release `{{ $labels.release }}` (`{{ $labels.chart }}` chart in namespace `{{ $labels.exported_namespace }}`) in version {{ $labels.version }} has not been ready for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-helmreleasenotdeployed"
          },
          "expr": "helm_chart_info != 1",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.release }}",
            "service": "helm-exporter"
          },
          "runbook": {
            "steps": [
              "Check the installed Helm releases via `helm --namespace monitoring ls --all`.",
              "If Helm cannot repair the chart automatically, delete/purge the chart (`helm delete --purge [RELEASE]`) and re-install the chart again."
            ]
          }
        }
      ]
    },
    {
      "name": "kube-apiserver",
      "rules": [
        {
          "alert": "KubernetesApiserverDown",
          "annotations": {
            "message": "KubernetesApiserver has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubernetesapiserverdown"
          },
          "expr": "absent(up{job=\"apiserver\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "apiserver",
            "service": "kubernetes"
          }
        },
        {
          "alert": "KubeAPITerminatedRequests",
          "annotations": {
            "message": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeapiterminatedrequests"
          },
          "expr": "sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))\n  /\n(sum(rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) ) > 0.20\n",
          "for": "5m",
          "labels": {
            "severity": "warning",
            "resource": "apiserver",
            "service": "kubernetes"
          }
        },
        {
          "alert": "KubeAPITerminatedRequests",
          "annotations": {
            "message": "The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeapiterminatedrequests"
          },
          "expr": "sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))\n  /\n(sum(rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) ) > 0.20\n",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "apiserver",
            "service": "kubernetes"
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "A client certificate used to authenticate to the apiserver is expiring in less than 7 days.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0\nand\nhistogram_quantile(0.01, sum by (job, instance, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 604800\n",
          "labels": {
            "severity": "warning",
            "resource": "apiserver",
            "service": "kubernetes"
          },
          "runbook": {
            "steps": [
              "Check the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) on how to renew certificates.",
              "If your certificate has already expired, the steps in the documentation might not work. Check [Github](https://github.com/kubernetes/kubeadm/issues/581#issuecomment-421477139) for hints about fixing your cluster."
            ]
          }
        },
        {
          "alert": "KubeClientCertificateExpiration",
          "annotations": {
            "message": "A client certificate used to authenticate to the apiserver is expiring in less than 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeclientcertificateexpiration"
          },
          "expr": "apiserver_client_certificate_expiration_seconds_count{job=\"apiserver\"} > 0\nand\nhistogram_quantile(0.01, sum by (job, instance, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m]))) < 86400\n",
          "labels": {
            "severity": "critical",
            "resource": "apiserver",
            "service": "kubernetes"
          },
          "runbook": {
            "steps": [
              "Urgently renew your certificates. Expired certificates can make fixing the cluster difficult to begin with.",
              "Check the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/) on how to renew certificates.",
              "If your certificate has already expired, the steps in the documentation might not work. Check [Github](https://github.com/kubernetes/kubeadm/issues/581#issuecomment-421477139) for hints about fixing your cluster."
            ]
          }
        }
      ]
    },
    {
      "name": "kube-kubelet",
      "rules": [
        {
          "alert": "KubeletDown",
          "annotations": {
            "message": "Kubelet has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeletdown"
          },
          "expr": "absent(up{job=\"kubelet\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeFillingUp",
          "annotations": {
            "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumefillingup"
          },
          "expr": "(\n  kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) < 0.05\nand\nkubelet_volume_stats_used_bytes{job=\"kubelet\"} > 0\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1\n",
          "for": "1m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeFillingUp",
          "annotations": {
            "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumefillingup"
          },
          "expr": "(\n  kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) < 0.07\nand\nkubelet_volume_stats_used_bytes{job=\"kubelet\"} > 0\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1\n",
          "for": "1m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeFillingUp",
          "annotations": {
            "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumefillingup"
          },
          "expr": "(\n  kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) < 0.15\nand\nkubelet_volume_stats_used_bytes{job=\"kubelet\"} > 0\nand\npredict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeInodesFillingUp",
          "annotations": {
            "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumeinodesfillingup"
          },
          "expr": "(\n  kubelet_volume_stats_inodes_free{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_inodes{job=\"kubelet\"}\n) < 0.03\nand\nkubelet_volume_stats_inodes_used{job=\"kubelet\"} > 0\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1\n",
          "for": "1m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeInodesFillingUp",
          "annotations": {
            "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumeinodesfillingup"
          },
          "expr": "(\n  kubelet_volume_stats_inodes_free{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_inodes{job=\"kubelet\"}\n) < 0.15\nand\nkubelet_volume_stats_inodes_used{job=\"kubelet\"} > 0\nand\npredict_linear(kubelet_volume_stats_inodes_free{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{ access_mode=\"ReadOnlyMany\"} == 1\nunless on(namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{label_excluded_from_alerts=\"true\"} == 1\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubePersistentVolumeErrors",
          "annotations": {
            "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-name-kubepersistentvolumeerrors"
          },
          "expr": "kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\"} > 0\n",
          "for": "5m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeletTooManyPods",
          "annotations": {
            "message": "Kubelet {{ $labels.instance }} is running {{ $value }} pods, close to the limit of 110.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubelettoomanypods"
          },
          "expr": "kubelet_running_pod_count{job=\"kubelet\"} > 110 * 0.9",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeletClientErrors",
          "annotations": {
            "message": "The kubelet on {{ $labels.instance }} is experiencing {{ printf \"%0.0f\" $value }}% errors.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeletclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"(5..|<error>)\",job=\"kubelet\"}[5m])) by (instance)\n  /\nsum(rate(rest_client_requests_total{job=\"kubelet\"}[5m])) by (instance))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeClientErrors",
          "annotations": {
            "message": "The pod {{ $labels.namespace }}/{{ $labels.pod }} is experiencing {{ printf \"%0.0f\" $value }}% errors.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeclienterrors"
          },
          "expr": "(sum(rate(rest_client_requests_total{code=~\"(5..|<error>)\",job=\"pods\"}[5m])) by (namespace, pod)\n  /\nsum(rate(rest_client_requests_total{job=\"pods\"}[5m])) by (namespace, pod))\n* 100 > 1\n",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeletRuntimeErrors",
          "annotations": {
            "message": "The kubelet on {{ $labels.instance }} is having an elevated error rate for container runtime operations.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeletruntimeerrors"
          },
          "expr": "sum(rate(kubelet_runtime_operations_errors_total{job=\"kubelet\"}[5m])) by (instance) > 0.1\n",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeletCGroupManagerDurationHigh",
          "annotations": {
            "message": "The kubelet's cgroup manager duration on {{ $labels.instance }} has been elevated ({{ printf \"%0.2f\" $value }}ms) for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeletcgroupmanagerlatencyhigh"
          },
          "expr": "sum(rate(kubelet_cgroup_manager_duration_seconds{quantile=\"0.9\"}[5m])) by (instance) * 1000 > 1\n",
          "for": "15m",
          "labels": {
            "resource": "{{ $labels.instance }}",
            "service": "kubelet",
            "severity": "warning"
          }
        },
        {
          "alert": "KubeletPodWorkerDurationHigh",
          "annotations": {
            "message": "The kubelet's pod worker duration for {{ $labels.operation_type }} operations on {{ $labels.instance }} has been elevated ({{ printf \"%0.2f\" $value }}ms) for more than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeletpodworkerdurationhigh"
          },
          "expr": "sum(rate(kubelet_pod_worker_duration_seconds{quantile=\"0.9\"}[5m])) by (instance, operation_type) * 1000 > 250\n",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }}/{{ $labels.operation_type }}",
            "service": "kubelet"
          }
        },
        {
          "alert": "KubeVersionMismatch",
          "annotations": {
            "message": "There are {{ $value }} different versions of Kubernetes components running.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeversionmismatch"
          },
          "expr": "count(count(kubernetes_build_info{job!=\"dns\"}) by (gitVersion)) > 1",
          "for": "1h",
          "labels": {
            "severity": "warning"
          }
        }
      ]
    },
    {
      "name": "kube-state-metrics",
      "rules": [
        {
          "alert": "KubeStateMetricsDown",
          "annotations": {
            "message": "KubeStateMetrics has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubestatemetricsdown"
          },
          "expr": "absent(up{job=\"kube-state-metrics\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.pod }}",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubePodCrashLooping",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubepodcrashlooping"
          },
          "expr": "max_over_time(kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\", job=\"kube-state-metrics\"}[5m]) >= 1",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.pod }}"
          },
          "runbook": {
            "steps": [
              "Check the pod's logs."
            ]
          }
        },
        {
          "alert": "KubePodNotReady",
          "annotations": {
            "message": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than an hour.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubepodnotready"
          },
          "expr": "sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}) > 0",
          "for": "30m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.pod }}"
          },
          "runbook": {
            "steps": [
              "Check the pod via `kubectl describe pod [POD]` to find out about scheduling issues."
            ]
          }
        },
        {
          "alert": "KubeDeploymentGenerationMismatch",
          "annotations": {
            "message": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubedeploymentgenerationmismatch"
          },
          "expr": "kube_deployment_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.deployment }}"
          }
        },
        {
          "alert": "KubeDeploymentReplicasMismatch",
          "annotations": {
            "message": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than an hour.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubedeploymentreplicasmismatch"
          },
          "expr": "kube_deployment_spec_replicas{job=\"kube-state-metrics\"}\n  !=\nkube_deployment_status_replicas_available{job=\"kube-state-metrics\"}\n",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.deployment }}"
          }
        },
        {
          "alert": "KubeStatefulSetReplicasMismatch",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubestatefulsetreplicasmismatch"
          },
          "expr": "kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_status_replicas{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.statefulset }}"
          }
        },
        {
          "alert": "KubeStatefulSetGenerationMismatch",
          "annotations": {
            "message": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubestatefulsetgenerationmismatch"
          },
          "expr": "kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_metadata_generation{job=\"kube-state-metrics\"}\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.statefulset }}"
          }
        },
        {
          "alert": "KubeStatefulSetUpdateNotRolledOut",
          "annotations": {
            "message": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubestatefulsetupdatenotrolledout"
          },
          "expr": "max without (revision) (\n  kube_statefulset_status_current_revision{job=\"kube-state-metrics\"}\n    unless\n  kube_statefulset_status_update_revision{job=\"kube-state-metrics\"}\n)\n  *\n(\n  kube_statefulset_replicas{job=\"kube-state-metrics\"}\n    !=\n  kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\"}\n)\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.statefulset }}"
          }
        },
        {
          "alert": "KubeDaemonSetRolloutStuck",
          "annotations": {
            "message": "Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubedaemonsetrolloutstuck"
          },
          "expr": "kube_daemonset_status_number_ready{job=\"kube-state-metrics\"}\n  /\nkube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"} * 100 < 100\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.namespace }}/{{ $labels.daemonset }}"
          }
        },
        {
          "alert": "KubeDaemonSetNotScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubedaemonsetnotscheduled"
          },
          "expr": "kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"}\n  -\nkube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"} > 0\n",
          "for": "10m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.daemonset }}"
          }
        },
        {
          "alert": "KubeDaemonSetMisScheduled",
          "annotations": {
            "message": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubedaemonsetmisscheduled"
          },
          "expr": "kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"} > 0",
          "for": "10m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.daemonset }}"
          }
        },
        {
          "alert": "KubeCronJobRunning",
          "annotations": {
            "message": "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubecronjobrunning"
          },
          "expr": "time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\"} > 3600",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.cronjob }}"
          }
        },
        {
          "alert": "KubeJobCompletion",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than one hour to complete.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubejobcompletion"
          },
          "expr": "time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{job=\"kube-state-metrics\"}\n  and\nkube_job_status_active{job=\"kube-state-metrics\"} > 0) > 43200\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.job_name }}"
          }
        },
        {
          "alert": "KubeJobFailed",
          "annotations": {
            "message": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubejobfailed"
          },
          "expr": "kube_job_status_failed{job=\"kube-state-metrics\"} > 0",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.job_name }}"
          }
        },
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.cpu\"})\n  /\nsum(node:node_num_cpu:sum)\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning",
            "resource": "cluster",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubeCPUOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubecpuovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n  -\n(sum(kube_node_status_allocatable{resource=\"cpu\"}) - max(kube_node_status_allocatable{resource=\"cpu\"}))\n  > 0\nand\n(sum(kube_node_status_allocatable{resource=\"cpu\"})\n  -\nmax(kube_node_status_allocatable{resource=\"cpu\"}))\n  > 0\n",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "cluster",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for namespaces.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubememovercommit"
          },
          "expr": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"requests.memory\"})\n  /\nsum(node_memory_MemTotal_bytes{app=\"node-exporter\"})\n  > 1.5\n",
          "for": "5m",
          "labels": {
            "severity": "warning",
            "resource": "cluster",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubeMemOvercommit",
          "annotations": {
            "message": "Cluster has overcommitted memory resource requests for Pods by {{ $value }} bytes and cannot tolerate node failure.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubememovercommit"
          },
          "expr": "sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n  -\n(sum(kube_node_status_allocatable{resource=\"memory\"}) - max(kube_node_status_allocatable{resource=\"memory\"}))\n  > 0\nand\n(sum(kube_node_status_allocatable{resource=\"memory\"})\n  -\nmax(kube_node_status_allocatable{resource=\"memory\"}))\n  > 0\n",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "cluster",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubeQuotaExceeded",
          "annotations": {
            "message": "Namespace {{ $labels.namespace }} is using {{ printf \"%0.0f\" $value }}% of its {{ $labels.resource }} quota.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubequotaexceeded"
          },
          "expr": "100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\"} > 0)\n  > 90\n",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cluster",
            "service": "kube-state-metrics"
          }
        },
        {
          "alert": "KubePodOOMKilled",
          "annotations": {
            "message": "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 30 minutes.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubepodoomkilled"
          },
          "expr": "(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 30m >= 2)\nand\nignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[30m]) == 1\n",
          "for": "0m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }}"
          }
        },
        {
          "alert": "KubeNodeNotReady",
          "annotations": {
            "message": "{{ $labels.node }} has been unready for more than an hour.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubenodenotready"
          },
          "expr": "kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.node }}"
          }
        }
      ]
    },
    {
      "name": "node-exporter",
      "rules": [
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemspacefillingup"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemSpaceFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of space within the next 4 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemspacefillingup"
          },
          "expr": "predict_linear(node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on node {{ $labels.node_name }} having IP {{ $labels.instance }} has only {{ $value }}% available space left on drive {{ $labels.device }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemoutofspace"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 10\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "30m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemOutOfSpace",
          "annotations": {
            "message": "Filesystem on node {{ $labels.node_name }} having IP {{ $labels.instance }} has only {{ $value }}% available space left on drive {{ $labels.device }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemoutofspace"
          },
          "expr": "node_filesystem_avail_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_size_bytes{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemFilesOutOfSpace",
          "annotations": {
            "message": "Filesystem on node {{ $labels.node_name }} having IP {{ $labels.instance }} has only {{ $value }}% inodes available on drive {{ $labels.device }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemfilesoutofspace"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 10\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemfilesfillingup"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 24*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.4\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemFilesFillingUp",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to run out of files within the next 4 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemfilesfillingup"
          },
          "expr": "predict_linear(node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"}[6h], 4*60*60) < 0\nand\nnode_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} < 0.2\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeFilesystemOutOfFiles",
          "annotations": {
            "message": "Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ $value }}% available inodes left.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodefilesystemoutoffiles"
          },
          "expr": "node_filesystem_files_free{app=\"node-exporter\",fstype=~\"ext.|xfs\"} / node_filesystem_files{app=\"node-exporter\",fstype=~\"ext.|xfs\"} * 100 < 5\nand\nnode_filesystem_readonly{app=\"node-exporter\",fstype=~\"ext.|xfs\"} == 0\n",
          "for": "1h",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeNetworkReceiveErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while receiving packets ({{ $value }} errors in two minutes).",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodenetworkreceiveerrs"
          },
          "expr": "increase(node_network_receive_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        },
        {
          "alert": "NodeNetworkTransmitErrs",
          "annotations": {
            "message": "{{ $labels.instance }} interface {{ $labels.device }} shows errors while transmitting packets ({{ $value }} errors in two minutes).",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-nodenetworktransmiterrs"
          },
          "expr": "increase(node_network_transmit_errs_total[2m]) > 10",
          "for": "1h",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.instance }} {{ $labels.device }}",
            "service": "node-exporter"
          }
        }
      ]
    },
    {
      "name": "prometheus",
      "rules": [
        {
          "alert": "PromScrapeFailed",
          "annotations": {
            "message": "Prometheus failed to scrape a target {{ $labels.job }} / {{ $labels.instance }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-promscrapefailed"
          },
          "expr": "up != 1",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable."
            ]
          }
        },
        {
          "alert": "PromBadConfig",
          "annotations": {
            "message": "Prometheus failed to reload config.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-prombadconfig"
          },
          "expr": "prometheus_config_last_reload_successful{job=\"prometheus\"} == 0",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Check the `prometheus-rules` configmap via `kubectl -n monitoring get configmap prometheus-rules -o yaml`."
            ]
          }
        },
        {
          "alert": "PromAlertmanagerBadConfig",
          "annotations": {
            "message": "Alertmanager failed to reload config.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-promalertmanagerbadconfig"
          },
          "expr": "alertmanager_config_last_reload_successful{job=\"alertmanager\"} == 0",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Check Alertmanager pod's logs via `kubectl -n monitoring logs alertmanager-0`, `-1` and `-2`.",
              "Check the `alertmanager` secret via `kubectl -n monitoring get secret alertmanager -o yaml`."
            ]
          }
        },
        {
          "alert": "PromAlertsFailed",
          "annotations": {
            "message": "Alertmanager failed to send an alert.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-promalertsfailed"
          },
          "expr": "sum(increase(alertmanager_notifications_failed_total{job=\"alertmanager\"}[5m])) by (namespace) > 0",
          "for": "5m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Make sure the Alertmanager StatefulSet is running: `kubectl -n monitoring get pods`."
            ]
          }
        },
        {
          "alert": "PromRemoteStorageFailures",
          "annotations": {
            "message": "Prometheus failed to send {{ printf \"%.1f\" $value }}% samples.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-promremotestoragefailures"
          },
          "expr": "(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) * 100)\n  /\n(rate(prometheus_remote_storage_failed_samples_total{job=\"prometheus\"}[1m]) + rate(prometheus_remote_storage_succeeded_samples_total{job=\"prometheus\"}[1m]))\n  > 1\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Ensure that the Prometheus volume has not reached capacity.",
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`."
            ]
          }
        },
        {
          "alert": "PromRuleFailures",
          "annotations": {
            "message": "Prometheus failed to evaluate {{ printf \"%.1f\" $value }} rules/sec.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-promrulefailures"
          },
          "expr": "rate(prometheus_rule_evaluation_failures_total{job=\"prometheus\"}[1m]) > 0",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.job }}/{{ $labels.instance }}",
            "service": "prometheus"
          },
          "runbook": {
            "steps": [
              "Check Prometheus pod's logs via `kubectl -n monitoring logs prometheus-0` and `-1`.",
              "Check CPU/memory pressure on the node."
            ]
          }
        }
      ]
    },
    {
      "name": "velero",
      "rules": [
        {
          "alert": "VeleroBackupTakesTooLong",
          "annotations": {
            "message": "Last backup with schedule {{ $labels.schedule }} has not finished successfully within 60min.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-velerobackuptakestoolong"
          },
          "expr": "time() - velero_backup_last_successful_timestamp{schedule!=\"\"} > 3600",
          "for": "5m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.schedule }}",
            "service": "velero"
          },
          "runbook": {
            "steps": [
              "Check if a backup is really in \"InProgress\" state via `velero -n velero backup get`.",
              "Check the backup logs via `velero -n velero backup logs [BACKUP_NAME]`.",
              "Depending on the backup, find the pod and check the processes inside that pod or any sidecar containers."
            ]
          }
        },
        {
          "alert": "VeleroNoRecentBackup",
          "annotations": {
            "message": "There has not been a successful backup for schedule {{ $labels.schedule }} in the last 24 hours.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-veleronorecentbackup"
          },
          "expr": "time() - velero_backup_last_successful_timestamp{schedule!=\"\"} > 3600*25",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.schedule }}",
            "service": "velero"
          },
          "runbook": {
            "steps": [
              "Check if really no backups happened via `velero -n velero backup get`.",
              "If a backup failed, check its logs via `velero -n velero backup logs [BACKUP_NAME]`.",
              "If a backup was not even triggered, check the Velero server's logs via `kubectl -n velero logs -l 'name=velero-server'`.",
              "Make sure the Velero server pod has not been rescheduled and possibly opt to schedule it on a stable node using a node affinity."
            ]
          }
        }
      ]
    },
    {
      "name": "kubermatic",
      "rules": [
        {
          "alert": "KubermaticAPIDown",
          "annotations": {
            "message": "KubermaticAPI has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticapidown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",app_kubernetes_io_name=\"kubermatic-api\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "service": "kubermatic-master"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable.",
              "Ensure that the API pod's logs and that it is not crashlooping."
            ]
          }
        },
        {
          "alert": "KubermaticAPITooManyErrors",
          "annotations": {
            "message": "Kubermatic API is returning a high rate of HTTP 5xx responses.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticapitoomanyerrors"
          },
          "expr": "sum(rate(http_requests_total{app_kubernetes_io_name=\"kubermatic-api\",code=~\"5..\"}[5m])) > 0.1",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "service": "kubermatic-master"
          },
          "runbook": {
            "steps": [
              "Check the API pod's logs."
            ]
          }
        },
        {
          "alert": "KubermaticAPITooManyInitNodeDeloymentFailures",
          "annotations": {
            "message": "Kubermatic API is failing to create too many initial node deployments."
          },
          "expr": "sum(rate(kubermatic_api_failed_init_node_deployment_total[5m])) > 0.01",
          "for": "15m",
          "labels": {
            "severity": "warning"
          }
        },
        {
          "alert": "KubermaticMasterControllerManagerDown",
          "annotations": {
            "message": "Kubermatic Master Controller Manager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticmastercontrollermanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",app_kubernetes_io_name=\"kubermatic-master-controller-manager\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "service": "kubermatic-master"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable.",
              "Ensure that the master-controller-manager pod's logs and that it is not crashlooping."
            ]
          }
        }
      ]
    },
    {
      "name": "kubermatic",
      "rules": [
        {
          "alert": "KubermaticTooManyUnhandledErrors",
          "annotations": {
            "message": "Kubermatic controller manager in {{ $labels.namespace }} is experiencing too many errors.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermatictoomanyunhandlederrors"
          },
          "expr": "sum(rate(kubermatic_controller_manager_unhandled_errors_total[5m])) > 0.01",
          "for": "10m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.namespace }}",
            "service": "kubermatic-seed"
          },
          "runbook": {
            "steps": [
              "Check the controller-manager pod's logs."
            ]
          }
        },
        {
          "alert": "KubermaticClusterDeletionTakesTooLong",
          "annotations": {
            "message": "Cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticclusterdeletiontakestoolong"
          },
          "expr": "(time() - max by (cluster) (kubermatic_cluster_deleted)) > 30*60",
          "for": "0m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.cluster }}",
            "service": "kubermatic-seed"
          },
          "runbook": {
            "steps": [
              "Check the machine-controller's logs via `kubectl -n cluster-XYZ logs -l 'app=machine-controller'` for errors related to cloud provider integrations. Expired credentials or manually deleted cloud provider resources are common reasons for failing deletions.",
              "Check the cluster's status itself via `kubectl describe cluster XYZ`.",
              "If all resources have been cleaned up, remove the blocking finalizer (e.g. `kubermatic.io/delete-nodes`) from the cluster resource.",
              "If nothing else helps, manually delete the cluster namespace as a last resort."
            ]
          }
        },
        {
          "alert": "KubermaticAddonDeletionTakesTooLong",
          "annotations": {
            "message": "Addon {{ $labels.addon }} in cluster {{ $labels.cluster }} is stuck in deletion for more than 30min.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticaddondeletiontakestoolong"
          },
          "expr": "(time() - max by (cluster,addon) (kubermatic_addon_deleted)) > 30*60",
          "for": "0m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.cluster }}/{{ $labels.addon }}",
            "service": "kubermatic-seed"
          },
          "runbook": {
            "steps": [
              "Check the kubermatic controller-manager's logs via `kubectl -n kubermatic logs -l 'app.kubernetes.io/name=kubermatic-seed-controller-manager'` for errors related to deletion of the addon. Manually deleted resources inside of the user cluster is a common reason for failing deletions.",
              "If all resources of the addon inside the user cluster have been cleaned up, remove the blocking finalizer (e.g. `cleanup-manifests`) from the addon resource."
            ]
          }
        },
        {
          "alert": "KubermaticAddonTakesTooLongToReconcile",
          "annotations": {
            "message": "Addon {{ $labels.addon }} in cluster {{ $labels.cluster }} has no related resources created for more than 30min.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticaddonreconciliationtakestoolong"
          },
          "expr": "kubermatic_addon_reconcile_failed * on(cluster) group_left() kubermatic_cluster_created\n- kubermatic_addon_reconcile_failed * on(cluster) group_left() kubermatic_cluster_deleted\n> 0\n",
          "for": "30m",
          "labels": {
            "severity": "warning",
            "resource": "{{ $labels.cluster }}/{{ $labels.addon }}",
            "service": "kubermatic-seed"
          },
          "runbook": {
            "steps": [
              "Check the kubermatic seed controller-manager's logs via `kubectl -n kubermatic logs -l 'app.kubernetes.io/name=kubermatic-seed-controller-manager'` for errors related to reconciliation of the addon."
            ]
          }
        },
        {
          "alert": "KubermaticSeedControllerManagerDown",
          "annotations": {
            "message": "Kubermatic Seed Controller Manager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubermaticseedcontrollermanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"kubermatic\",app_kubernetes_io_name=\"kubermatic-seed-controller-manager\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "service": "kubermatic-seed"
          },
          "runbook": {
            "steps": [
              "Check the Prometheus Service Discovery page to find out why the target is unreachable.",
              "Ensure that the seed-controller-manager pod's logs and that it is not crashlooping."
            ]
          }
        },
        {
          "alert": "OpenVPNServerDown",
          "annotations": {
            "message": "There is no healthy OpenVPN server in cluster {{ $labels.cluster }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-openvpnserverdown"
          },
          "expr": "(kube_deployment_status_replicas_available{cluster!=\"\",deployment=\"openvpn-server\"} != kube_deployment_spec_replicas{cluster!=\"\",deployment=\"openvpn-server\"}) and count(kubermatic_cluster_info) > 0",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.cluster }}",
            "service": "kubermatic-seed"
          }
        },
        {
          "alert": "UserClusterPrometheusAbsent",
          "annotations": {
            "message": "There is no Prometheus in cluster {{ $labels.name }}.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-userclusterprometheusdisappeared"
          },
          "expr": "(\n  kubermatic_cluster_info * on (name) group_left\n  label_replace(up{job=\"clusters\"}, \"name\", \"$1\", \"namespace\", \"cluster-(.+)\")\n  or\n  kubermatic_cluster_info * 0\n) == 0\n",
          "for": "15m",
          "labels": {
            "severity": "critical",
            "resource": "{{ $labels.name }}",
            "service": "kubermatic-seed"
          }
        },
        {
          "alert": "KubermaticClusterPaused",
          "annotations": {
            "message": "Cluster {{ $labels.name }} has been paused and will not be reconciled until the pause flag is reset."
          },
          "expr": "label_replace(kubermatic_cluster_info{pause=\"true\"}, \"cluster\", \"$0\", \"name\", \".+\")",
          "labels": {
            "severity": "informational",
            "resource": "{{ $labels.name }}",
            "service": "kubermatic-seed"
          }
        }
      ]
    },
    {
      "name": "kube-controller-manager",
      "rules": [
        {
          "alert": "KubeControllerManagerDown",
          "annotations": {
            "message": "No healthy controller-manager pods exist inside the cluster.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubecontrollermanagerdown"
          },
          "expr": "absent(:ready_kube_controller_managers:sum) or :ready_kube_controller_managers:sum == 0",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "kube-controller-manager",
            "service": "kubernetes"
          }
        }
      ]
    },
    {
      "name": "kube-scheduler",
      "rules": [
        {
          "alert": "KubeSchedulerDown",
          "annotations": {
            "message": "No healthy scheduler pods exist inside the cluster.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-kubeschedulerdown"
          },
          "expr": "absent(:ready_kube_schedulers:sum) or :ready_kube_schedulers:sum == 0",
          "for": "10m",
          "labels": {
            "severity": "critical",
            "resource": "kube-scheduler",
            "service": "kubernetes"
          }
        }
      ]
    },
    {
      "name": "cortex",
      "rules": [
        {
          "alert": "CortexDistributorDown",
          "annotations": {
            "message": "Cortex-distributor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexdistributordown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"distributor\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexQuerierDown",
          "annotations": {
            "message": "Cortex-querier has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexquerierdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"querier\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexQueryFrontendDown",
          "annotations": {
            "message": "Cortex-query-frontend has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexqueryfrontenddown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"query-frontend\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexRulerDown",
          "annotations": {
            "message": "Cortex-ruler has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexrulerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"ruler\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexMemcachedBlocksDown",
          "annotations": {
            "message": "Cortex-memcached-blocks has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexmemcachedblocksdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\", app_kubernetes_io_instance=\"cortex\",app_kubernetes_io_name=\"memcached-blocks\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexMemcachedBlocksMetadataDown",
          "annotations": {
            "message": "Cortex-memcached-blocks-metadata has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexmemcachedblocksmetadatadown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_instance=\"cortex\",app_kubernetes_io_name=\"memcached-blocks-metadata\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexMemcachedBlocksIndexDown",
          "annotations": {
            "message": "Cortex-memcached-blocks-index has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexmemcachedblocksindexdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_instance=\"cortex\",app_kubernetes_io_name=\"memcached-blocks-index\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexAlertmanagerDown",
          "annotations": {
            "message": "Cortex-alertmanager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexalertmanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"alertmanager\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexCompactorDown",
          "annotations": {
            "message": "Cortex-compactor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexcompactordown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"compactor\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexIngesterDown",
          "annotations": {
            "message": "Cortex-ingester has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexingesterdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"ingester\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        },
        {
          "alert": "CortexStoreGatewayDown",
          "annotations": {
            "message": "Cortex-store-gateway has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-cortexstoregatewaydown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"store-gateway\",app_kubernetes_io_name=\"cortex\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "cortex",
            "service": "cortex"
          }
        }
      ]
    },
    {
      "name": "loki-distributed",
      "rules": [
        {
          "alert": "LokiIngesterDown",
          "annotations": {
            "message": "Loki-ingester has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokiingesterdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"ingester\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiDistributorDown",
          "annotations": {
            "message": "Loki-distributor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokidistributordown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"distributor\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiQuerierDown",
          "annotations": {
            "message": "Loki-querier has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokiquerierdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"querier\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiQueryFrontendDown",
          "annotations": {
            "message": "Loki-query-frontend has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokiqueryfrontenddown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"query-frontend\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiTableManagerDown",
          "annotations": {
            "message": "Loki-table-manager has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokitablemanagerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"table-manager\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiCompactorDown",
          "annotations": {
            "message": "Loki-compactor has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokicompactordown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"compactor\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiRulerDown",
          "annotations": {
            "message": "Loki-ruler has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokirulerdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"ruler\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiMemcachedChunksDown",
          "annotations": {
            "message": "Loki-memcached-chunks has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokimemcachedchunksdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"memcached-chunks\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiMemcachedFrontendDown",
          "annotations": {
            "message": "Loki-memcached-frontend has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokimemcachedfrontenddown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"memcached-frontend\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiMemcachedIndexQueriesDown",
          "annotations": {
            "message": "Loki-memcached-index-queries has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokimemcachedindexqueriesdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"memcached-index-queries\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        },
        {
          "alert": "LokiMemcachedIndexWritesDown",
          "annotations": {
            "message": "Loki-memcached-index-writes has disappeared from Prometheus target discovery.",
            "runbook_url": "https://docs.kubermatic.com/kubermatic/main/cheat-sheets/alerting-runbook/#alert-lokimemcachedindexwritesdown"
          },
          "expr": "absent(up{job=\"pods\",namespace=\"mla\",app_kubernetes_io_component=\"memcached-index-writes\",app_kubernetes_io_name=\"loki-distributed\"} == 1)",
          "for": "15m",
          "labels": {
            "severity": "warning",
            "resource": "loki",
            "service": "loki"
          }
        }
      ]
    }
  ]
}
